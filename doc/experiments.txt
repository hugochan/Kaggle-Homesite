## How to handle mixed data (numerical & categorical):
1) LabelEncoder, OneHotEncoder...
2) etc.

## Feature extraction
1) PCA +  Individual relevance ranking
2) LDA
3) FAMD
4) IRR
5) standardization
6) etc.

## Model selection
1) xgboost
2) RF
3) logistic regression
4) naive bayes
5) neural network
6) etc.

## Experiment results
1) Feature engineering

xgboost settings:
                            n_estimators=25,
                            nthread=-1,
                            max_depth=10,
                            learning_rate=0.02,
                            silent=True,
                            subsample=0.8,
                            colsample_bytree=0.6

| No. | Fillnan -1 | Fillnan mean | LableEncoder | OneHotEncoder | Standardized | PCA | LDA | l1_based_select | xgboost |      cv-3 auc        | runtime (s)
   1       T                            T                                                                          T           0.95887889176       354
   2       T                            T                                          T                               T           0.935672327087      647
   3       T                            T                               T                                          T           0.958878835725      330
   4       T                            T                               T          T                               T           0.924474693345      639
   5                     T              T                                                                          T           0.9588768449837     528
   6       T                                            T                                                          T           0.958892573508      780 *
   7       T                                            T               T                T                         T           0.957453567302      778   
   8       T                                            T                                            T             T           0.957124130844      1831
   9 count nans and 0s



6 - 1, Date process: Day, 0.957334358422
6 - 2, Date process: Year + Day, 0.957379036551
6 - 3, Date process: Year + YDay, 0.95740636219 (YDay: day in year)
6 - 4ï¼ŒDate process: Year + Month + weekday + Day + YDay, 0.957416102677
6 - 5, Date process: Year + Month + MDay + weekday, 0.95737584241 (MDay: day in month)
Date process here does not change much.


## Note: 
1) Fillnan median/most frequent does not change much. Fillnan -1 is simple and good.
2) Tuning model params makes large difference.   


2) Model selection

(a) xgboost

| No. | n_estimators | max_depth | min_child_weight | learning_rate | subsample | colsample_bytree | scale_pos_weight |   cv-3 auc          | runtime (s)
   1         25           10             1                0.025          0.8           1                    0.2           0.953122491788        
   2         25           10             1                0.025          0.8           1                    0.5           0.956502826059        
   3         25           10             1                0.025          0.8           1                    0.8           0.957082065222        
   4         25           10             1                0.025          0.8           1                    1 *           0.958892573508        780   - 
   5         80           10             1                0.025          0.8           1                    1             0.960112662974        
   6         200          10             1                0.025          0.8           1                    1             0.963575538026        39971
   7         25           14             1                0.025          0.8           1                    1             0.960261598989        1485
   8         25           10             1                0.03 *         0.8           1                    1             0.960519979029        1431
   9         25           10             1                0.04           0.8           1                    1             0.958303510628        918
   10        25           10             1                0.035          0.8           1                    1             0.958086848026        909
   11        25           16 *           1                0.03           0.8           1                    1             0.960587075228        1316
   12        25           16             1                0.03           0.8           0.8                  1             0.959121192637        819
   13        25           18             1                0.03           0.8           1                    1             0.960382557821        1516
   14        25           16             2 *              0.03           0.8           1                    1             0.960706412226        4072
   15        25           17             2                0.03           0.8           1                    1             0.960559414833        4177
   16        25           16             3                0.03           0.8           1                    1             0.960698434604        4055
   17        25           16             2                0.03           0.8           0.95                 1             0.961174355307        1390
   18        25           16             2                0.03           0.8           0.9                  1             0.961279863443        4002
   19        25           16             2                0.03           0.8           0.85                 1             0.961647077321        1264
   20        25           16             2                0.03           0.8           0.83                 1             0.961752583248        1256
   21        25           16             2                0.03           0.9           0.85                 1             0.96167622779         1275
   22        25           16             2                0.03           0.95          0.83                 1             0.961690391163        1065
   23        25           16             2                0.03           0.85          0.83                 1             0.961754581925        1205
   24        25           16             2                0.03           0.9           0.83                 1             0.961739095261        1077
   25        25           16             2                0.03           0.83 *        0.83                 1             0.961806175163        1212
   26        25           16             2                0.03           0.82          0.83                 1             0.961706461213        1215
   27        25           16             2                0.03           0.8           0.82                 1             0.961753015015        1182
  *28        25           16             2                0.03           0.8           0.81 *               1             0.961839284312        2308
   29        25           17 **          1                0.03           0.8           1                    1             0.960594601059        4014
   30        25           16             2                0.03           0.84          0.83                 1             0.961717141883        1237
   31        25           16             2                0.03           0.83          0.81                 1             0.961781749503        1048
   32        80           16             2                0.03           0.8           0.81                 1             0.963366514018        3571
   33        200          16             2                0.03           0.8           0.81                 1             0.964816480523        7868 
   34        400          16             2                0.03           0.8           0.81                 1             0.965476805491        22655 *  
   35        80           16             2                0.03           0.83          0.81                 1             0.963302645587        5565  
   36        80           16             2                0.03           0.83          0.83                 1             0.963230755938        5314  
 m-37        25           16             1                0.03           0.8           0.8                  1             0.961632878357        1198
 m-38        25           16             2                0.03           0.8           0.81                 1             0.961833301408        1154


# note: with min_child_weight set, gamma param seems not to be "useful".

Cont'd

gamma
  0        0.961839284312
  0.05     0.961839450838        1267
  0.2      0.961815645201        1201
  0.3      0.961840982194        1166

colsample_bylevel
    1                 0.961839284312  
    0.95              0.961498992978    1016
    0.9               0.961678572084    969
    0.8               0.961596928525    843


selected params set:
 
| No. | n_estimators | max_depth | min_child_weight | learning_rate | subsample | colsample_bytree | scale_pos_weight
   1         25           16             2                0.03           0.83          0.81                 1
   2         25           16             2                0.03           0.83          0.83                 1
   3         25           16             2                0.03           0.8           0.81                 1




(b) logistic regression

| No. | penalty |   tol   | C | solver | max_iter |  cv-3 auc      | runtime (s)
   1       l2     0.0001    1     sag     10000      0.8181932195      22926

(c) naive bayes

| No. |  cv-3 auc        | runtime (s)
   1     0.753509641813      98

(d) neural network 

# note: label_encoder works much better here.

## Layer information

  #  name        size
---  --------  ------
  0  input        612
  1  dense0       200
  2  dropout0     200
  3  output         2

n_epoch:5

cv-3 auc: 0.951791492455
runtime (s): 370


n_epoch: 10
cv-3 auc: 0.95176460659
runtime (s): 400
runtime ()_


## submission
1) OneHotEncoder + xgboost (n=80)
   LB score: 0.96426
   runtime (s): N/A

2) OneHotEncoder + xgboost (n=200)
   LB score: 0.96575
   runtime (s): 5941
3) OneHotEncoder + xgboost (n=400)
   LB score: 0.96644
   runtime (s): 9701
4) OneHotEncoder + xgboost (n=800)
   LB score: 0.96645
   runtime (s): N/A
5) OneHotEncoder + Centered + xgboost (n=500)
   LB score: 0.96659
   runtime (s): < 12432

6) Xgboost_stop_R (eta = 0.023, max_depth = 6, subsample = 0.83, colsample_bytree = 0.77, nrounds = 3000)
   LB score: 0.96809

7) Xgboost_stop_R (eta = 0.023, max_depth = 6, subsample = 0.83, colsample_bytree = 0.77, nrounds = 3000) + addNullCol
   LB score: 0.96814
